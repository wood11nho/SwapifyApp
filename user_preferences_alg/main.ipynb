{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVD approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import firestore\n",
    "\n",
    "# We should create a dictionary, and to have for exampel dict[userId] = [itemInteractions, postedItems, searchHistory], where\n",
    "# itemInteractions, postedItems, searchHistory are arrays of strings\n",
    "\n",
    "# Get all documents from USER_PREFERENCES collection\n",
    "db = firestore.Client(project='swapify-e426d')\n",
    "\n",
    "collection_name = 'USER_PREFERENCES'\n",
    "docs = db.collection(collection_name).stream()\n",
    "\n",
    "# Process Firestore data\n",
    "user_and_preference_data = []\n",
    "for doc in docs:\n",
    "    user_id = doc.id\n",
    "    data = doc.to_dict()\n",
    "    preferences = set(data.get('itemInteractions', []) + data.get('postedItems', []) + data.get('searchHistory', []))\n",
    "    for preference in preferences:\n",
    "        user_and_preference_data.append([user_id, preference])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Pece4uaUoFeRxlKegA19vRpaUhO2', 'shoes'], ['Pece4uaUoFeRxlKegA19vRpaUhO2', 'Funko Pops Stranger Things'], ['Pece4uaUoFeRxlKegA19vRpaUhO2', 'Tricou Alb Simplu'], ['Pece4uaUoFeRxlKegA19vRpaUhO2', 'Funko Pops and Figures'], ['Pece4uaUoFeRxlKegA19vRpaUhO2', 'Tricou Borussia Dortmund'], ['Pece4uaUoFeRxlKegA19vRpaUhO2', 'Bucharest'], ['Pece4uaUoFeRxlKegA19vRpaUhO2', 'Others'], ['Pece4uaUoFeRxlKegA19vRpaUhO2', 'Retro kits'], ['Pece4uaUoFeRxlKegA19vRpaUhO2', 'Pantofi Nike Air Max'], ['Pece4uaUoFeRxlKegA19vRpaUhO2', 'Shoes'], ['RtukkaH4z0VxgMOd8ePBpR425ro2', 'se vinde, 80 de lei, doar f2f in Galati'], ['RtukkaH4z0VxgMOd8ePBpR425ro2', 'Vreau sa ofer acest tricou cuiva care are nevoie de el. Am multe astfel de produse, astept mesaje pentru doritori. Banii stransi vor fi donati unei cauze, multumesc frumos. Astept mesaj in privat pentru detalii!'], ['RtukkaH4z0VxgMOd8ePBpR425ro2', 'Vand tricou retro cu borussia dortmund, original, cu eticheta, marimea M. Trimit in tara sau f2f in Brasov.'], ['RtukkaH4z0VxgMOd8ePBpR425ro2', 'Vreau sa ofer acest tricou pentru suma modica de 20 RON. Am foarte multe produse in stilul acesta, ieftine dar nepurtate. Vreau sa donez toti banii unei cauze. Astept mesaj pentru detalii!!!'], ['RtukkaH4z0VxgMOd8ePBpR425ro2', 'Vand figurine Funko Pop, colectia Stranger Things. Fiecare figurina are pretul fix de 100 RON. Trimit in tara/F2F Galati'], ['pr2VSj0MnpPDzR6lBmc0BfDiqeV2', 'test test'], ['pr2VSj0MnpPDzR6lBmc0BfDiqeV2', 'Accept trade pentru figurinele din imagine']]\n"
     ]
    }
   ],
   "source": [
    "# Print data\n",
    "print(user_and_preference_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preference  0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15  16\n",
      "user_id                                                                       \n",
      "0            0   1   1   1   1   1   1   1   1   1   0   0   0   0   0   1   0\n",
      "1            0   0   0   0   0   0   0   0   0   0   1   1   1   1   1   0   0\n",
      "2            1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1\n"
     ]
    }
   ],
   "source": [
    "# Convert to dataframe\n",
    "df = pd.DataFrame(user_and_preference_data, columns=['user_id', 'preference'])\n",
    "\n",
    "user_encoder = LabelEncoder()\n",
    "preference_encoder = LabelEncoder()\n",
    "\n",
    "df['user_id'] = user_encoder.fit_transform(df['user_id'])\n",
    "df['preference'] = preference_encoder.fit_transform(df['preference'])\n",
    "\n",
    "# Make interaction matrix from dataframe (user_id, preference)\n",
    "interaction_matrix = df.pivot_table(index='user_id', columns='preference', aggfunc=len, fill_value=0)\n",
    "print(interaction_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 15)\t1\n",
      "  (1, 10)\t1\n",
      "  (1, 11)\t1\n",
      "  (1, 12)\t1\n",
      "  (1, 13)\t1\n",
      "  (1, 14)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 16)\t1\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Make sparse matrix from interaction matrix (pairs of user_id, preference and their number of interactions)\n",
    "sparse_interaction_matrix = csr_matrix(interaction_matrix.values)\n",
    "print(sparse_interaction_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preference   0    1    2    3    4    5    6    7    8    9    10   11   12  \\\n",
      "user_id                                                                       \n",
      "0           0.0  1.0  4.0  4.0  8.0  3.0  2.0  3.0  8.0  2.0  0.0  0.0  0.0   \n",
      "1           0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0  1.0   \n",
      "2           1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "\n",
      "preference   13   14   15   16  \n",
      "user_id                         \n",
      "0           0.0  0.0  1.0  0.0  \n",
      "1           1.0  1.0  0.0  0.0  \n",
      "2           0.0  0.0  0.0  3.0  \n",
      "  (0, 1)\t1.0\n",
      "  (0, 2)\t4.0\n",
      "  (0, 3)\t4.0\n",
      "  (0, 4)\t8.0\n",
      "  (0, 5)\t3.0\n",
      "  (0, 6)\t2.0\n",
      "  (0, 7)\t3.0\n",
      "  (0, 8)\t8.0\n",
      "  (0, 9)\t2.0\n",
      "  (0, 15)\t1.0\n",
      "  (1, 10)\t1.0\n",
      "  (1, 11)\t1.0\n",
      "  (1, 12)\t1.0\n",
      "  (1, 13)\t1.0\n",
      "  (1, 14)\t1.0\n",
      "  (2, 0)\t1.0\n",
      "  (2, 16)\t3.0\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Get the interactions and their count, so we can make like a rating system based on the number of interactions\n",
    "docs = db.collection(collection_name).stream()\n",
    "\n",
    "user_and_preferences_counter = Counter()\n",
    "\n",
    "for doc in docs:\n",
    "    user_id = doc.id\n",
    "    data = doc.to_dict()\n",
    "    preferences = data.get('itemInteractions', []) + data.get('postedItems', []) + data.get('searchHistory', [])\n",
    "    for preference in preferences:\n",
    "        user_and_preferences_counter[(user_id, preference)] += 1\n",
    "\n",
    "user_and_preference_data_with_count = [[user_id, item, count] for (user_id, item), count in user_and_preferences_counter.items()]\n",
    "df_with_count = pd.DataFrame(user_and_preference_data_with_count, columns=['user_id', 'preference', 'count'])\n",
    "\n",
    "df_with_count['user_id'] = user_encoder.fit_transform(df_with_count['user_id'])\n",
    "df_with_count['preference'] = preference_encoder.fit_transform(df_with_count['preference'])\n",
    "\n",
    "interaction_matrix_with_count = df_with_count.pivot_table(index='user_id', columns='preference', values='count', fill_value=0)\n",
    "print(interaction_matrix_with_count)\n",
    "\n",
    "sparse_interaction_matrix_with_count = csr_matrix(interaction_matrix_with_count.values)\n",
    "print(sparse_interaction_matrix_with_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Dataset, Reader\n",
    "from surprise import SVD\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Prediction(uid=0, iid=5, r_ui=3.0, est=2.5585533848167357, details={'was_impossible': False}), Prediction(uid=1, iid=10, r_ui=1.0, est=1.882102428870836, details={'was_impossible': False}), Prediction(uid=0, iid=8, r_ui=8.0, est=2.5585533848167357, details={'was_impossible': False}), Prediction(uid=0, iid=2, r_ui=4.0, est=2.5585533848167357, details={'was_impossible': False})]\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "# Convert the dataset into the specific format that Surprise uses\n",
    "coo = coo_matrix(sparse_interaction_matrix_with_count)\n",
    "df_for_surprise = pd.DataFrame({'user_id': coo.row, 'preference': coo.col, 'rating': coo.data})\n",
    "df_for_surprise = df_for_surprise[df_for_surprise['rating'] > 0]\n",
    "\n",
    "# Load the dataset into Surprise\n",
    "reader = Reader(rating_scale=(1, df_for_surprise['rating'].max()))\n",
    "data = Dataset.load_from_df(df_for_surprise[['user_id', 'preference', 'rating']], reader)\n",
    "\n",
    "# Split the dataset into train and test\n",
    "trainset, testset = train_test_split(data, test_size=0.2)\n",
    "\n",
    "# Train the model\n",
    "algo = SVD()\n",
    "algo.fit(trainset)\n",
    "\n",
    "# Predict the rating for a specific user and item\n",
    "predictions = algo.test(testset)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 2.8575\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.857450324125919"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from surprise import accuracy\n",
    "\n",
    "accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF and cosine similarity approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\stoic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\stoic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\stoic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mulţi', 'nostri', 'atatia', 'curier', 'ţi', 'acela', 'deasupra', 'cine', 'ăla', 'oricând', 'către', 'sai', 'mie', 'îţi', 'alti', 'la', 'cea', 'tău', 'ceva', 'pe', 'prin', 'aceeasi', 'toata', 'în', 'pînă', 'lui', 'uneori', 'il', 'meet', 'dintre', 'care', 'oricât', 'astept', 'up', 'ele', 'alea', 'avut', 'cât', 'toti', 'acestea', 'ia', 'ul', 'sa', 'atat', 'mă', 'fac', 'v', 'pot', 'apoi', 'anume', 'ca', 'contact', 'caruia', 'pentru', 'fim', 'al', 'in', 'acest', 'până', 'altceva', 'suntem', 'vor', 'fiţi', 'patru', 'cite', 'vreun', 'ea', 'unde', 'fara', 'tine', 'eu', 'aceste', 'aveţi', 'deşi', 'fără', 'cita', 'mîine', 'cei', 'ălea', 'dintr-', 'or', 'inainte', 'mâine', 'aceia', 'multa', 'pai', 'livrare', 'catre', 'este', 'iti', 'detalii', 'de', 'te', 'doar', 'insa', 'voi', 'cînd', 'mesaje', 'fel', 'cit', 'mele', 'meu', 'pic', 'deja', 'sau', 'nou', 'si', 'voastră', 'acestei', 'deci', 'ni', 'nimeni', 'citi', 'ii', 'noastră', 'vand', 'asa', 'le', 'au', 'dă', 'tăi', 'mereu', 'imi', 'alt', 'se', 'sintem', 'marimea', 'această', 'acea', 'foarte', 'sunteţi', 'oricine', 'undeva', 'intr', 'ta', 'numai', 'dau', 'doilea', 'noştri', 'uneia', 'vi', 'fi', 'toată', 'vouă', 'eşti', 'careia', 'chiar', 'ala', 'dupa', 'voastre', 'mesaj', 'unei', 'nouă', 'patra', 'cumva', 'ce', 'zice', 'vreau', 'atit', 'atunci', 'săi', 'e', 'prima', 'vedeti', 'celor', 'iar', 'ceilalti', 'citiva', 'aceea', 'schimb', 'acesta', 'trimit', 'nişte', 'pina', 'noi', 'cu', 'telefon', 'carora', 'asupra', 'voştri', 'unuia', 'vom', 'ăstea', 'avea', 'inapoi', 'nimic', 'are', 'toate', 'trei', 'său', 'daca', 'ale', 'zi', 'şi', 'asta', 'astfel', 'face', 'despre', 'să', 'ti', 'nici', 'as', 'oriunde', 'atatea', 'acelasi', 'atita', 'personala', 'cîţi', 'sunt', 'am', 'fata', 'dat', 'sa-ti', 'unele', 'fiu', 'tara', 'mi', 'câţi', 'intre', 'inca', 'nostru', 'ba', 'aţi', 'lîngă', 'treilea', 'multă', 'mai', 'mea', 'mod', 'da', 't', 'cît', 'acelea', 'atare', 'unui', 'căci', 'nu', 'cărei', 'unu', 'acestia', 'el', 'tuturor', 'm', 'oricînd', 'oricare', 'atata', 'oricît', 'sale', 'dată', 'totusi', 'mei', 'aceasta', 'aici', 'putini', 'toţi', 'unii', 'dintr', 'poate', 'cat', 'u', 'cam', 'sus', 'două', 'aş', 'citeva', 'multi', 'avem', 'ăsta', 'unora', 'parca', 'acestui', 'cîte', 'lor', 'ori', 'lângă', 'unul', 'cineva', 'sigilat', 'privat', 'după', 'cum', 'spate', 'acel', 'cîtva', 'noua', 'acei', 'sub', 'ului', 'treia', 'fost', 'aceştia', 'acele', 'cel', 'căror', 'câte', 'puţină', 'ma', 'predare', 'multe', 'atitia', 'altcineva', 'aibă', 'niste', 'dar', 'ţie', 'o', 'una', 'îmi', 'cand', 'vă', 'mult', 'decit', 'astea', 'ăştia', 'tot', 'a', 'adica', 'primul', 'că', 'mine', 'alte', 'dacă', 'ne', 'peste', 'doi', 'ati', 'ei', 'f2f', 'desi', 'fiecare', 'eram', 'alta', 'totuşi', 'un', 'era', 'tu', 'altfel', 'îl', 'vreo', 'cui', 'isi', 'tale', 'cind', 'schimbare', 'ai', 'drept', 'aia', 'câtva', 'îi', 'printr-', 'vostru', 'tocmai', 'altii', 'oricum', 'abia', 'ar', 'ci', 'schimburi', 'va', 'bucuresti', 'prea', 'sa-mi', 'din', 'aceşti', 'pm', 'unor', 'i', 'sint', 'orice', 'cărui', 'puţin', 'puţina', 'altul', 'cele', 'incit', 'ceea', 'fii', 'atitea', 'spre', 'totul', 'noastre', 'buna', 'fie', 'li'}\n",
      "['accept trade figurinele', '', 'funko pop stranger', 'funko pop and', '', 'pantofi nike air', 'retro', '', 'tricou alb', 'tricou borussia', 'figurine funko pop colectia stranger thing figurina pretul fix ron', 'tricou retro borussia dortmund original eticheta brasov', 'ofer tricou cuiva nevoie produse doritori banii stransi donati cauze multumesc frumos', 'ofer tricou suma modica ron produse stilul ieftine nepurtate donez banii cauze', 'vinde lei', '', 'test']\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data for the model\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, MWETokenizer\n",
    "import re\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('romanian'))\n",
    "custom_stop_words = {'trimit', 'curier', 'livrare', 'privat', 'mesaj', 'contact', 'detalii', \n",
    "                     'telefon', 'meet', 'up', 'pm', 'predare', 'personala', 'f2f', 'mesaje', 'tara', 'bucuresti',\n",
    "                     'sigilat', 'marimea', 'astept', 'buna', 'vreau', 'sa', 'vand', 'schimb', 'schimburi', 'schimbare',\n",
    "                     'fac', 'vedeti'}\n",
    "stop_words = stop_words.union(custom_stop_words)\n",
    "\n",
    "print(stop_words)\n",
    "\n",
    "tokenized_preferences = [word_tokenize(preference.lower()) for preference in preference_encoder.classes_]\n",
    "\n",
    "all_tokens = [token for sublist in tokenized_preferences for token in sublist]\n",
    "\n",
    "bigram_measures = BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(all_tokens)\n",
    "finder.apply_freq_filter(5)\n",
    "scored = finder.score_ngrams(bigram_measures.likelihood_ratio)\n",
    "\n",
    "phrases = set([' '.join(bigram) for bigram, score in scored if score >= 0.5])\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [' '.join(tokens[i:i+2]) if ' '.join(tokens[i:i+2]) in phrases else tokens[i] for i in range(len(tokens)-1)]\n",
    "    processed_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words and word.isalpha()]\n",
    "    return ' '.join(processed_tokens)\n",
    "\n",
    "preprocessed_preferences = [preprocess_text(preference) for preference in preference_encoder.classes_]\n",
    "print(preprocessed_preferences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['shoes', 'Funko Pops Stranger Things', 'test test', 'Tricou Alb Simplu', 'Funko Pops and Figures', 'Tricou Borussia Dortmund', 'Bucharest', 'Others', 'se vinde, 80 de lei, doar f2f in Galati', 'Retro kits', 'Vreau sa ofer acest tricou cuiva care are nevoie de el. Am multe astfel de produse, astept mesaje pentru doritori. Banii stransi vor fi donati unei cauze, multumesc frumos. Astept mesaj in privat pentru detalii!', 'Accept trade pentru figurinele din imagine', 'Vand tricou retro cu borussia dortmund, original, cu eticheta, marimea M. Trimit in tara sau f2f in Brasov.', 'Pantofi Nike Air Max', 'Vreau sa ofer acest tricou pentru suma modica de 20 RON. Am foarte multe produse in stilul acesta, ieftine dar nepurtate. Vreau sa donez toti banii unei cauze. Astept mesaj pentru detalii!!!', 'Vand figurine Funko Pop, colectia Stranger Things. Fiecare figurina are pretul fix de 100 RON. Trimit in tara/F2F Galati', 'Shoes']\n",
      "['', 'funko pop stranger', 'test', 'tricou alb', 'funko pop and', 'tricou borussia', '', '', 'vinde lei', 'retro', 'ofer tricou cuiva nevoie produse doritori banii stransi donati cauze multumesc frumos', 'accept trade figurinele', 'tricou retro borussia dortmund original eticheta brasov', 'pantofi nike air', 'ofer tricou suma modica ron produse stilul ieftine nepurtate donez banii cauze', 'figurine funko pop colectia stranger thing figurina pretul fix ron', '']\n"
     ]
    }
   ],
   "source": [
    "# We take all the queries/items/interactions from the dataset\n",
    "all_preference_interactions = list(set([preference for _, preference in user_and_preference_data]))\n",
    "print(all_preference_interactions)\n",
    "preprocessed_all_preference_interactions = [preprocess_text(preference) for preference in all_preference_interactions]\n",
    "print(preprocessed_all_preference_interactions)\n",
    "\n",
    "# We create a TF-IDF model\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_model = tfidf.fit(preprocessed_all_preference_interactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preference_string(preference_id):\n",
    "    return preference_encoder.inverse_transform([preference_id])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: array([0.        , 0.04811252, 0.18577687, 0.07445869, 0.        ,\n",
      "       0.04440782, 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.12001643, 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.04811252, 0.        , 0.        ,\n",
      "       0.04811252, 0.12001643, 0.        , 0.        , 0.05555556,\n",
      "       0.        , 0.        , 0.06878857, 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.15532337, 0.        ]), 1: array([0.        , 0.        , 0.        , 0.        , 0.10850944,\n",
      "       0.07155947, 0.08195251, 0.10850944, 0.06760574, 0.06177835,\n",
      "       0.06177835, 0.06249061, 0.06177835, 0.08195251, 0.08195251,\n",
      "       0.06760574, 0.06760574, 0.        , 0.06760574, 0.06177835,\n",
      "       0.05294905, 0.06249061, 0.14142136, 0.06249061, 0.06177835,\n",
      "       0.06249061, 0.06177835, 0.        , 0.10850944, 0.08195251,\n",
      "       0.        , 0.05294905, 0.06760574, 0.10850944, 0.07155947,\n",
      "       0.11359781, 0.06249061, 0.05903212, 0.06177835, 0.06249061,\n",
      "       0.        , 0.06760574, 0.        , 0.13536081, 0.14142136]), 2: array([0.14433757, 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.14433757, 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.75      , 0.        , 0.14433757, 0.        , 0.        ])}\n"
     ]
    }
   ],
   "source": [
    "user_profiles = {}\n",
    "\n",
    "for user_id in set(df['user_id']):\n",
    "    # Get preferences and counts for the user\n",
    "    user_preferences = df_with_count[df_with_count['user_id'] == user_id]\n",
    "    user_preferences_tfidf = np.zeros((tfidf_model.transform(['']).shape[1],))\n",
    "    \n",
    "    # Loop through each preference and add its weighted TF-IDF vector\n",
    "    for _, row in user_preferences.iterrows():\n",
    "        preference = get_preference_string(row['preference'])\n",
    "        count = row['count']\n",
    "        tfidf_vector = tfidf_model.transform([preprocess_text(preference)]).toarray()\n",
    "        user_preferences_tfidf += tfidf_vector[0] * count\n",
    "    \n",
    "    # Normalize the TF-IDF vector by the total count of interactions\n",
    "    total_count = user_preferences['count'].sum()\n",
    "    if total_count > 0:\n",
    "        user_preferences_tfidf /= total_count\n",
    "    \n",
    "    user_profiles[user_id] = user_preferences_tfidf\n",
    "\n",
    "print(user_profiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tricou fotbal ronaldo retro kit tricou fotbal ronaldo real madrid sezonul l culoare albastru\n"
     ]
    }
   ],
   "source": [
    "new_item_name = \"Tricou Fotbal Ronaldo\"\n",
    "new_item_category = \"Retro kits\"\n",
    "new_item_description = \"Tricou Fotbal Ronaldo Real Madrid sezonul 2018-2019, marimea L, culoare albastru, trimit si in tara\"\n",
    "\n",
    "new_item_text = new_item_name + \" \" + new_item_category + \" \" + new_item_description\n",
    "preprocessed_new_item_text = preprocess_text(new_item_text)\n",
    "\n",
    "print(preprocessed_new_item_text)\n",
    "new_item_vector = tfidf.transform([preprocessed_new_item_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "funko pop star war the mandalorian the child funko pop and figure funko pop star war the mandalorian the child\n"
     ]
    }
   ],
   "source": [
    "new_item_name2 = \"Funko Pop! Star Wars: The Mandalorian - The Child\"\n",
    "new_item_category2 = \"Funko Pops and Figures\"\n",
    "new_item_description2 = \"Funko Pop! Star Wars: The Mandalorian - The Child, nou, sigilat, trimit si in tara\"\n",
    "\n",
    "new_item_text2 = new_item_name2 + \" \" + new_item_category2 + \" \" + new_item_description2\n",
    "preprocessed_new_item_text2 = preprocess_text(new_item_text2)\n",
    "\n",
    "print(preprocessed_new_item_text2)\n",
    "new_item_vector2 = tfidf.transform([preprocessed_new_item_text2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.4833104464200692, 1: 0.30961076166630996, 2: 0.0}\n"
     ]
    }
   ],
   "source": [
    "user_interest_predictions = {}\n",
    "\n",
    "for user_id, profile in user_profiles.items():\n",
    "    similarity = cosine_similarity([profile], new_item_vector)\n",
    "    user_interest_predictions[user_id] = similarity[0][0]\n",
    "\n",
    "print(user_interest_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.5554324192872526, 1: 0.14573244072709848, 2: 0.0}\n"
     ]
    }
   ],
   "source": [
    "user_interest_predictions2 = {}\n",
    "\n",
    "for user_id, profile in user_profiles.items():\n",
    "    similarity = cosine_similarity([profile], new_item_vector2)\n",
    "    user_interest_predictions2[user_id] = similarity[0][0]\n",
    "    \n",
    "print(user_interest_predictions2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
