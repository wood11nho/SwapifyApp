{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVD approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 787,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 788,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import firestore\n",
    "\n",
    "# We should create a dictionary, and to have for exampel dict[userId] = [itemInteractions, postedItems, searchHistory], where\n",
    "# itemInteractions, postedItems, searchHistory are arrays of strings\n",
    "\n",
    "# Get all documents from USER_PREFERENCES collection\n",
    "db = firestore.Client(project='swapify-e426d')\n",
    "\n",
    "collection_name = 'USER_PREFERENCES'\n",
    "docs = db.collection(collection_name).stream()\n",
    "\n",
    "# Process Firestore data\n",
    "user_and_preference_data = []\n",
    "for doc in docs:\n",
    "    user_id = doc.id\n",
    "    data = doc.to_dict()\n",
    "    preferences = set(data.get('itemInteractions', []) + data.get('postedItems', []) + data.get('searchHistory', []))\n",
    "    for preference in preferences:\n",
    "        user_and_preference_data.append([user_id, preference])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 789,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['gxRY52w1CMhuzG8fwkPNuiO1FER2', 'Shoes'], ['gxRY52w1CMhuzG8fwkPNuiO1FER2', 'Vand Tricou cu FC Liverpool, Fernando Torres 9, Marimea M, Livrare prin curier sau meet up in Bucuresti'], ['gxRY52w1CMhuzG8fwkPNuiO1FER2', 'tricouri fotbal\\n'], ['gxRY52w1CMhuzG8fwkPNuiO1FER2', 'Vand pantofi Nike, Marimea 39, Astept mesaje in privat. Nu trimit pe curier'], ['gxRY52w1CMhuzG8fwkPNuiO1FER2', 'Funko Pops and Figures'], ['gxRY52w1CMhuzG8fwkPNuiO1FER2', 'Tricou Liverpool Torres'], ['gxRY52w1CMhuzG8fwkPNuiO1FER2', 'Buna! Vreau sa fac trade cu alte Funko pentru ce vedeti in imagine. PM '], ['gxRY52w1CMhuzG8fwkPNuiO1FER2', 'Retro kits']]\n"
     ]
    }
   ],
   "source": [
    "# Print data\n",
    "print(user_and_preference_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 790,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preference  0  1  2  3  4  5  6  7\n",
      "user_id                           \n",
      "0           1  1  1  1  1  1  1  1\n"
     ]
    }
   ],
   "source": [
    "# Convert to dataframe\n",
    "df = pd.DataFrame(user_and_preference_data, columns=['user_id', 'preference'])\n",
    "\n",
    "user_encoder = LabelEncoder()\n",
    "preference_encoder = LabelEncoder()\n",
    "\n",
    "df['user_id'] = user_encoder.fit_transform(df['user_id'])\n",
    "df['preference'] = preference_encoder.fit_transform(df['preference'])\n",
    "\n",
    "# Make interaction matrix from dataframe (user_id, preference)\n",
    "interaction_matrix = df.pivot_table(index='user_id', columns='preference', aggfunc=len, fill_value=0)\n",
    "print(interaction_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 791,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 7)\t1\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Make sparse matrix from interaction matrix (pairs of user_id, preference and their number of interactions)\n",
    "sparse_interaction_matrix = csr_matrix(interaction_matrix.values)\n",
    "print(sparse_interaction_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 792,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preference    0    1    2    3    4    5    6    7\n",
      "user_id                                           \n",
      "0           1.0  2.0  6.0  1.0  3.0  5.0  1.0  1.0\n",
      "  (0, 0)\t1.0\n",
      "  (0, 1)\t2.0\n",
      "  (0, 2)\t6.0\n",
      "  (0, 3)\t1.0\n",
      "  (0, 4)\t3.0\n",
      "  (0, 5)\t5.0\n",
      "  (0, 6)\t1.0\n",
      "  (0, 7)\t1.0\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Get the interactions and their count, so we can make like a rating system based on the number of interactions\n",
    "docs = db.collection(collection_name).stream()\n",
    "\n",
    "user_and_preferences_counter = Counter()\n",
    "\n",
    "for doc in docs:\n",
    "    user_id = doc.id\n",
    "    data = doc.to_dict()\n",
    "    preferences = data.get('itemInteractions', []) + data.get('postedItems', []) + data.get('searchHistory', [])\n",
    "    for preference in preferences:\n",
    "        user_and_preferences_counter[(user_id, preference)] += 1\n",
    "\n",
    "user_and_preference_data_with_count = [[user_id, item, count] for (user_id, item), count in user_and_preferences_counter.items()]\n",
    "df_with_count = pd.DataFrame(user_and_preference_data_with_count, columns=['user_id', 'preference', 'count'])\n",
    "\n",
    "df_with_count['user_id'] = user_encoder.fit_transform(df_with_count['user_id'])\n",
    "df_with_count['preference'] = preference_encoder.fit_transform(df_with_count['preference'])\n",
    "\n",
    "interaction_matrix_with_count = df_with_count.pivot_table(index='user_id', columns='preference', values='count', fill_value=0)\n",
    "print(interaction_matrix_with_count)\n",
    "\n",
    "sparse_interaction_matrix_with_count = csr_matrix(interaction_matrix_with_count.values)\n",
    "print(sparse_interaction_matrix_with_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 793,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Dataset, Reader\n",
    "from surprise import SVD\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 794,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Prediction(uid=0, iid=0, r_ui=1.0, est=2.3414925540526372, details={'was_impossible': False}), Prediction(uid=0, iid=5, r_ui=5.0, est=2.3414925540526372, details={'was_impossible': False})]\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "# Convert the dataset into the specific format that Surprise uses\n",
    "coo = coo_matrix(sparse_interaction_matrix_with_count)\n",
    "df_for_surprise = pd.DataFrame({'user_id': coo.row, 'preference': coo.col, 'rating': coo.data})\n",
    "df_for_surprise = df_for_surprise[df_for_surprise['rating'] > 0]\n",
    "\n",
    "# Load the dataset into Surprise\n",
    "reader = Reader(rating_scale=(1, df_for_surprise['rating'].max()))\n",
    "data = Dataset.load_from_df(df_for_surprise[['user_id', 'preference', 'rating']], reader)\n",
    "\n",
    "# Split the dataset into train and test\n",
    "trainset, testset = train_test_split(data, test_size=0.2)\n",
    "\n",
    "# Train the model\n",
    "algo = SVD()\n",
    "algo.fit(trainset)\n",
    "\n",
    "# Predict the rating for a specific user and item\n",
    "predictions = algo.test(testset)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 795,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 2.1056\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.105619162234263"
      ]
     },
     "execution_count": 795,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from surprise import accuracy\n",
    "\n",
    "accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF and cosine similarity approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 796,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pina', 'treia', 'treilea', 'deja', 'cineva', 'acea', 'oricine', 'unul', 'căci', 'vi', 'aceea', 'altfel', 'vreo', 'insa', 'sint', 'atunci', 'schimb', 'pe', 'vedeti', 'pot', 'mult', 'personala', 'mea', 'i', 'mesaj', 'deasupra', 'acei', 'avea', 'am', 'atita', 'alti', 'face', 'multe', 'vom', 'predare', 'îţi', 'aş', 'până', 'această', 'sai', 'u', 'ti', 'buna', 'aceştia', 'lîngă', 'lor', 'acesta', 'as', 'schimburi', 'abia', 'doi', 'spre', 'cu', 'un', 'ţi', 'in', 'mod', 'asta', 'ţie', 'oricînd', 'acele', 'multa', 'ei', 'nouă', 'acelea', 'toata', 'foarte', 'citiva', 'trimit', 'totusi', 'dă', 't', 'sale', 'ceilalti', 'nou', 'vor', 'acestia', 'desi', 'puţina', 'atitia', 'ălea', 'mulţi', 'ta', 'dintr-', 'altceva', 'mi', 'una', 'unii', 'ci', 'suntem', 'ce', 'cărei', 'de', 'o', 'doar', 'îl', 'tăi', 'tău', 'iar', 'daca', 'caruia', 'voi', 'atare', 'voştri', 'ia', 'ii', 'celor', 'meet', 'unde', 'în', 'up', 'le', 'fara', 'se', 'noştri', 'cita', 'fac', 'ori', 'zice', 'da', 'eşti', 'prea', 'este', 'altcineva', 'mâine', 'ceea', 'mereu', 'ne', 'cât', 'ăştia', 'au', 'uneia', 'sub', 'nici', 'dintr', 'ea', 'curier', 'unu', 'lui', 'privat', 'niste', 'fata', 'unuia', 'sa-mi', 'anume', 'eram', 'trei', 'oricît', 'sunteţi', 'alte', 'totul', 'unei', 'cam', 'mîine', 'ăsta', 'oricând', 'cand', 'aceşti', 'te', 'căror', 'cît', 'drept', 'mie', 'îmi', 'noi', 'zi', 'cumva', 'vouă', 'cea', 'doilea', 'nimic', 'aceeasi', 'aceasta', 'peste', 'iti', 'orice', 'il', 'schimbare', 'vă', 'sus', 'cărui', 'catre', 'cîte', 'aceste', 'tara', 'noastră', 'tot', 'fiţi', 'prin', 'ăla', 'm', 'aţi', 'câte', 'si', 'astept', 'altii', 'era', 'vand', 'alea', 'sau', 'sunt', 'voastră', 'intr', 'puţină', 'sa-ti', 'patru', 'cine', 'cele', 'noastre', 'cui', 'multă', 'către', 'al', 'printr-', 'unor', 'multi', 'mă', 'la', 'fel', 'toate', 'lângă', 'astea', 'prima', 'sigilat', 'dupa', 'inapoi', 'oricare', 'vreau', 'dau', 'dintre', 'unui', 'putini', 'cum', 'vostru', 'ăstea', 'vreun', 'atatea', 'mai', 'telefon', 'alt', 'apoi', 'ca', 'ele', 'cel', 'îi', 'ala', 'fiu', 'mesaje', 'el', 'cat', 'cîţi', 'asupra', 'carora', 'atat', 'acel', 'tu', 'a', 'altul', 'ni', 'săi', 'toţi', 'fii', 'aveţi', 'eu', 'ati', 'fie', 'li', 'acestui', 'primul', 'asa', 'acestei', 'totuşi', 'tale', 'atatia', 'adica', 'ale', 'careia', 'fiecare', 'bucuresti', 'contact', 'ar', 'sintem', 'din', 'ba', 'nişte', 'acela', 'care', 'aia', 'astfel', 'şi', 'mele', 'tocmai', 'fost', 'oricât', 'câtva', 'isi', 'mine', 'aici', 'său', 'mei', 'uneori', 'pentru', 'dat', 'undeva', 'sa', 'f2f', 'cind', 'cîtva', 'cite', 'v', 'acelasi', 'livrare', 'deşi', 'unele', 'despre', 'două', 'oricum', 'fără', 'citi', 'atata', 'poate', 'chiar', 'pai', 'intre', 'oriunde', 'e', 'alta', 'pic', 'atitea', 'deci', 'fim', 'parca', 'nostru', 'or', 'inainte', 'patra', 'după', 'voastre', 'nostri', 'atit', 'dacă', 'aceia', 'incit', 'cînd', 'ul', 'dar', 'va', 'ai', 'tine', 'avut', 'aibă', 'spate', 'marimea', 'pînă', 'are', 'numai', 'toată', 'unora', 'nu', 'noua', 'tuturor', 'cit', 'puţin', 'toti', 'ului', 'acestea', 'fi', 'avem', 'cei', 'imi', 'inca', 'să', 'câţi', 'decit', 'dată', 'ma', 'meu', 'pm', 'citeva', 'nimeni', 'ceva', 'detalii', 'acest', 'că'}\n",
      "['trade funko imagine', 'funko pop and figure', 'retro kit', 'shoe', 'tricou liverpool torres', 'tricou fc liverpool fernando torres', 'pantofi nike', 'tricouri fotbal']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\stoic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\stoic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\stoic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data for the model\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, MWETokenizer\n",
    "import re\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('romanian'))\n",
    "custom_stop_words = {'trimit', 'curier', 'livrare', 'privat', 'mesaj', 'contact', 'detalii', \n",
    "                     'telefon', 'meet', 'up', 'pm', 'predare', 'personala', 'f2f', 'mesaje', 'tara', 'bucuresti',\n",
    "                     'sigilat', 'marimea', 'astept', 'buna', 'vreau', 'sa', 'vand', 'schimb', 'schimburi', 'schimbare',\n",
    "                     'fac', 'vedeti'}\n",
    "stop_words = stop_words.union(custom_stop_words)\n",
    "\n",
    "print(stop_words)\n",
    "\n",
    "tokenized_preferences = [word_tokenize(preference.lower()) for preference in preference_encoder.classes_]\n",
    "phrases = Phrases(tokenized_preferences, min_count=5, threshold=0.5, scoring='npmi')\n",
    "phraser = Phraser(phrases)\n",
    "    \n",
    "def preprocess_text(text):\n",
    "    # No longer removing numbers\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = phraser[tokens]\n",
    "    processed_tokens = [lemmatizer.lemmatize(word.lower()) for word in tokens if word.lower() not in stop_words and word.isalpha()]\n",
    "    return ' '.join(processed_tokens)\n",
    "\n",
    "preprocessed_preferences = [preprocess_text(preference) for preference in preference_encoder.classes_]\n",
    "print(preprocessed_preferences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 797,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 798,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Shoes', 'Vand Tricou cu FC Liverpool, Fernando Torres 9, Marimea M, Livrare prin curier sau meet up in Bucuresti', 'tricouri fotbal\\n', 'Vand pantofi Nike, Marimea 39, Astept mesaje in privat. Nu trimit pe curier', 'Funko Pops and Figures', 'Tricou Liverpool Torres', 'Buna! Vreau sa fac trade cu alte Funko pentru ce vedeti in imagine. PM ', 'Retro kits']\n",
      "['shoe', 'tricou fc liverpool fernando torres', 'tricouri fotbal', 'pantofi nike', 'funko pop and figure', 'tricou liverpool torres', 'trade funko imagine', 'retro kit']\n"
     ]
    }
   ],
   "source": [
    "# We take all the queries/items/interactions from the dataset\n",
    "all_preference_interactions = list(set([preference for _, preference in user_and_preference_data]))\n",
    "print(all_preference_interactions)\n",
    "preprocessed_all_preference_interactions = [preprocess_text(preference) for preference in all_preference_interactions]\n",
    "print(preprocessed_all_preference_interactions)\n",
    "\n",
    "# We create a TF-IDF model\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_model = tfidf.fit(preprocessed_all_preference_interactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 799,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preference_string(preference_id):\n",
    "    return preference_encoder.inverse_transform([preference_id])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 800,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: array([0.05197085, 0.12335907, 0.12335907, 0.05197085, 0.03535534,\n",
      "       0.06904632, 0.03041566, 0.21213203, 0.18998707, 0.03535534,\n",
      "       0.03535534, 0.05197085, 0.21213203, 0.05      , 0.18998707,\n",
      "       0.03041566, 0.18998707, 0.03535534])}\n"
     ]
    }
   ],
   "source": [
    "user_profiles = {}\n",
    "\n",
    "for user_id in set(df['user_id']):\n",
    "    # Get preferences and counts for the user\n",
    "    user_preferences = df_with_count[df_with_count['user_id'] == user_id]\n",
    "    user_preferences_tfidf = np.zeros((tfidf_model.transform(['']).shape[1],))\n",
    "    \n",
    "    # Loop through each preference and add its weighted TF-IDF vector\n",
    "    for _, row in user_preferences.iterrows():\n",
    "        preference = get_preference_string(row['preference'])\n",
    "        count = row['count']\n",
    "        tfidf_vector = tfidf_model.transform([preprocess_text(preference)]).toarray()\n",
    "        user_preferences_tfidf += tfidf_vector[0] * count\n",
    "    \n",
    "    # Normalize the TF-IDF vector by the total count of interactions\n",
    "    total_count = user_preferences['count'].sum()\n",
    "    if total_count > 0:\n",
    "        user_preferences_tfidf /= total_count\n",
    "    \n",
    "    user_profiles[user_id] = user_preferences_tfidf\n",
    "\n",
    "print(user_profiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 801,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tricou fotbal ronaldo retro kit tricou fotbal ronaldo real madrid sezonul l culoare albastru\n"
     ]
    }
   ],
   "source": [
    "new_item_name = \"Tricou Fotbal Ronaldo\"\n",
    "new_item_category = \"Retro kits\"\n",
    "new_item_description = \"Tricou Fotbal Ronaldo Real Madrid sezonul 2018-2019, marimea L, culoare albastru, trimit si in tara\"\n",
    "\n",
    "new_item_text = new_item_name + \" \" + new_item_category + \" \" + new_item_description\n",
    "preprocessed_new_item_text = preprocess_text(new_item_text)\n",
    "\n",
    "print(preprocessed_new_item_text)\n",
    "new_item_vector = tfidf.transform([preprocessed_new_item_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 802,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "funko pop star war the mandalorian the child funko pop and figure funko pop star war the mandalorian the child\n"
     ]
    }
   ],
   "source": [
    "new_item_name2 = \"Funko Pop! Star Wars: The Mandalorian - The Child\"\n",
    "new_item_category2 = \"Funko Pops and Figures\"\n",
    "new_item_description2 = \"Funko Pop! Star Wars: The Mandalorian - The Child, nou, sigilat, trimit si in tara\"\n",
    "\n",
    "new_item_text2 = new_item_name2 + \" \" + new_item_category2 + \" \" + new_item_description2\n",
    "preprocessed_new_item_text2 = preprocess_text(new_item_text2)\n",
    "\n",
    "print(preprocessed_new_item_text2)\n",
    "new_item_vector2 = tfidf.transform([preprocessed_new_item_text2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 803,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 804,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.547085414066346}\n"
     ]
    }
   ],
   "source": [
    "user_interest_predictions = {}\n",
    "\n",
    "for user_id, profile in user_profiles.items():\n",
    "    similarity = cosine_similarity([profile], new_item_vector)\n",
    "    user_interest_predictions[user_id] = similarity[0][0]\n",
    "\n",
    "print(user_interest_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 805,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.20790510890286867}\n"
     ]
    }
   ],
   "source": [
    "user_interest_predictions2 = {}\n",
    "\n",
    "for user_id, profile in user_profiles.items():\n",
    "    similarity = cosine_similarity([profile], new_item_vector2)\n",
    "    user_interest_predictions2[user_id] = similarity[0][0]\n",
    "    \n",
    "print(user_interest_predictions2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
